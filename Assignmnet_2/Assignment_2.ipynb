{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "82e7877d",
      "metadata": {
        "id": "82e7877d"
      },
      "source": [
        "# Assignmnet 2 (100 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "297b6d20",
      "metadata": {
        "id": "297b6d20"
      },
      "source": [
        "**Name:** <br>\n",
        "**Email:** <br>\n",
        "**Group:** A/B <br>\n",
        "**Hours spend *(optional)* :** <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09f79f88",
      "metadata": {
        "id": "09f79f88"
      },
      "source": [
        "### SMS Spam Detection *(60 points)*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "148e88d0",
      "metadata": {
        "id": "148e88d0"
      },
      "source": [
        "<p>You are hired as an AI expert in the development department of a telecommunications company. The first thing on your orientation plan is a small project that your boss has assigned you for the following given situation. Your supervisor has given away his private cell phone number on too many websites and is now complaining about daily spam SMS. Therefore, it is your job to write a spam detector in Python. </p>\n",
        "\n",
        "<p>In doing so, you need to use a Naive Bayes classifier that can handle both bag-of-words (BoW) and tf-idf features as input. For the evaluation of your spam detector, an SMS collection is available as a dataset - this has yet to be suitably split into train and test data. To keep the costs as low as possible and to avoid problems with copyrights, your boss insists on a new development with Python.</p>\n",
        "\n",
        "<p>Include a short description of the data preprocessing steps, method, experiment design, hyper-parameters, and evaluation metric. Also, document your findings, drawbacks, and potential improvements.</p>\n",
        "\n",
        "<p>Note: You need to implement the bag-of-words (BoW) and tf-idf feature extractor from scratch. You can use existing python libraries for other tasks.</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fad12eba",
      "metadata": {
        "id": "fad12eba"
      },
      "source": [
        "**Dataset and Resources**\n",
        "\n",
        "* SMS Spam Collection Dataset: https://archive.ics.uci.edu/dataset/228/sms+spam+collection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "f4109920",
      "metadata": {
        "id": "f4109920"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Results using BOW:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.99      1.00      0.99       966\n",
            "        spam       1.00      0.92      0.96       149\n",
            "\n",
            "    accuracy                           0.99      1115\n",
            "   macro avg       0.99      0.96      0.98      1115\n",
            "weighted avg       0.99      0.99      0.99      1115\n",
            "\n",
            "\n",
            "Results using TFIDF:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.96      1.00      0.98       966\n",
            "        spam       1.00      0.72      0.84       149\n",
            "\n",
            "    accuracy                           0.96      1115\n",
            "   macro avg       0.98      0.86      0.91      1115\n",
            "weighted avg       0.96      0.96      0.96      1115\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import math\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Step 1: Load and preprocess data\n",
        "df = pd.read_csv(r\"sms_spam_collection\\SMSSpamCollection.csv\", sep='\\t', header=None, names=['label', 'message'])\n",
        "\n",
        "\n",
        "\n",
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    return text.split()\n",
        "\n",
        "df['tokens'] = df['message'].apply(preprocess)\n",
        "\n",
        "# Step 2: Bag of Words implementation\n",
        "def bag_of_words(docs):\n",
        "    vocabulary = {}\n",
        "    bow_vectors = []\n",
        "    idx = 0\n",
        "    for tokens in docs:\n",
        "        for token in tokens:\n",
        "            if token not in vocabulary:\n",
        "                vocabulary[token] = idx\n",
        "                idx += 1\n",
        "    for tokens in docs:\n",
        "        vec = [0] * len(vocabulary)\n",
        "        for token in tokens:\n",
        "            vec[vocabulary[token]] += 1\n",
        "        bow_vectors.append(vec)\n",
        "    return bow_vectors, vocabulary\n",
        "\n",
        "# Step 3: TF-IDF implementation\n",
        "def tf_idf(docs, vocabulary=None, df_count=None, existing_vocab=False):\n",
        "    N = len(docs)\n",
        "    if vocabulary is None:\n",
        "        vocabulary = {}\n",
        "    if df_count is None:\n",
        "        df_count = defaultdict(int)\n",
        "\n",
        "    tfidf_vectors = []\n",
        "    idx = len(vocabulary)\n",
        "\n",
        "    # Count doc frequency only if not reusing vocab\n",
        "    if not existing_vocab:\n",
        "        for tokens in docs:\n",
        "            for token in set(tokens):\n",
        "                df_count[token] += 1\n",
        "                if token not in vocabulary:\n",
        "                    vocabulary[token] = idx\n",
        "                    idx += 1\n",
        "\n",
        "    for tokens in docs:\n",
        "        vec = [0] * len(vocabulary)\n",
        "        token_counts = defaultdict(int)\n",
        "        for token in tokens:\n",
        "            token_counts[token] += 1\n",
        "        for token, count in token_counts.items():\n",
        "            if token in vocabulary:\n",
        "                tf = count / len(tokens)\n",
        "                idf = math.log(N / (df_count[token] + 1))\n",
        "                vec[vocabulary[token]] = tf * idf\n",
        "        tfidf_vectors.append(vec)\n",
        "    return tfidf_vectors, vocabulary, df_count\n",
        "\n",
        "# Step 4: Naive Bayes Classifier\n",
        "class NaiveBayesClassifier:\n",
        "    def fit(self, X, y):\n",
        "        self.classes = list(set(y))\n",
        "        self.class_probs = {}\n",
        "        self.word_probs = {}\n",
        "        for c in self.classes:\n",
        "            X_c = [X[i] for i in range(len(y)) if y[i] == c]\n",
        "            total_words = np.sum(X_c, axis=0)\n",
        "            class_total = np.sum(total_words)\n",
        "            self.class_probs[c] = len(X_c) / len(y)\n",
        "            self.word_probs[c] = (np.array(total_words) + 1) / (class_total + len(total_words))\n",
        "\n",
        "    def predict(self, X):\n",
        "        preds = []\n",
        "        for x in X:\n",
        "            class_scores = {}\n",
        "            for c in self.classes:\n",
        "                log_prob = np.log(self.class_probs[c])\n",
        "                log_prob += np.sum(x * np.log(self.word_probs[c]))\n",
        "                class_scores[c] = log_prob\n",
        "            preds.append(max(class_scores, key=class_scores.get))\n",
        "        return preds\n",
        "\n",
        "# Step 5: Run Experiment\n",
        "def run_experiment(feature_type='bow'):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(df['tokens'], df['label'], test_size=0.2, random_state=42)\n",
        "\n",
        "    if feature_type == 'bow':\n",
        "        X_train_vec, vocab = bag_of_words(X_train)\n",
        "        X_test_vec = [[0] * len(vocab) for _ in X_test]\n",
        "        for i, tokens in enumerate(X_test):\n",
        "            for token in tokens:\n",
        "                if token in vocab:\n",
        "                    X_test_vec[i][vocab[token]] += 1\n",
        "\n",
        "    elif feature_type == 'tfidf':\n",
        "        # Train TF-IDF with new vocab\n",
        "        X_train_vec, vocab, df_count = tf_idf(X_train)\n",
        "\n",
        "        # Reuse vocab and doc freq for test\n",
        "        X_test_vec, _, _ = tf_idf(X_test, vocabulary=vocab, df_count=df_count, existing_vocab=True)\n",
        "\n",
        "    model = NaiveBayesClassifier()\n",
        "    model.fit(X_train_vec, list(y_train))\n",
        "    preds = model.predict(X_test_vec)\n",
        "\n",
        "    print(f\"\\nResults using {feature_type.upper()}:\")\n",
        "    print(classification_report(y_test, preds))\n",
        "\n",
        "\n",
        "\n",
        "# Run both experiments\n",
        "run_experiment('bow')\n",
        "run_experiment('tfidf')\n",
        "\n",
        "\n",
        "# ðŸ“„ **SMS Spam Detection: Summary Report**\n",
        "#\n",
        "# **Data Preprocessing:**\n",
        "# - **Lowercasing**: All text is converted to lowercase.\n",
        "# - **Noise Removal**: Non-alphabetic characters (punctuation, digits, etc.) are removed.\n",
        "# - **Tokenization**: The message is split into a list of words (tokens).\n",
        "#\n",
        "# **Methodology:**\n",
        "# - Two feature extraction methods: **Bag of Words (BoW)** and **TF-IDF (Term Frequency-Inverse Document Frequency)**.\n",
        "# - A custom **Naive Bayes Classifier** is used to classify the messages as spam or ham.\n",
        "#\n",
        "# **Experiment Design:**\n",
        "# - **Train/Test Split**: 80% training, 20% testing.\n",
        "# - **Features Used**: Bag of Words and TF-IDF.\n",
        "# - **Workflow**:\n",
        "#   1. Preprocess the messages.\n",
        "#   2. Vectorize the tokens using BoW or TF-IDF.\n",
        "#   3. Train the classifier on the training data.\n",
        "#   4. Evaluate performance on test data.\n",
        "#\n",
        "# **Hyperparameters:**\n",
        "# - **Laplace Smoothing (Î±)**: Implicitly applied by adding 1 to word counts.\n",
        "# - **TF-IDF IDF Smoothing**: Added +1 in the denominator to avoid division by zero.\n",
        "#\n",
        "# **Evaluation Metric**:\n",
        "# - **Classification Report**: Includes accuracy, precision, recall, and F1-score for both classes (ham and spam).\n",
        "#\n",
        "# **Findings:**\n",
        "# - **BoW** and **TF-IDF** both achieved high accuracy (~97%).\n",
        "# - **BoW** slightly outperforms **TF-IDF** in this case due to the small size of the dataset.\n",
        "#\n",
        "# **Drawbacks:**\n",
        "# - **No Stopwords Removal**: Common words like \"the\", \"is\" may dilute the signal.\n",
        "# - **Limited Tokenization**: Misses advanced tokenization (e.g., contractions, negations).\n",
        "# - **Vocabulary Size Mismatch**: The vocab size differs between BoW and TF-IDF, leading to shape mismatches.\n",
        "# - **Naive Bayes Limitation**: Assumes word independence, which is not always true.\n",
        "#\n",
        "# **Potential Improvements:**\n",
        "# - Use **TfidfVectorizer** or **CountVectorizer** from sklearn for better preprocessing and vectorization handling.\n",
        "# - Add **stopword removal** and **lemmatization**.\n",
        "# - Test more advanced models (e.g., logistic regression, decision trees).\n",
        "# - Use **N-grams** for better contextual understanding of words."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jEykmdVPwqPA",
      "metadata": {
        "id": "jEykmdVPwqPA"
      },
      "source": [
        " ### Search Engine *(40 points)*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-CRJj4Ypw1Z2",
      "metadata": {
        "id": "-CRJj4Ypw1Z2"
      },
      "source": [
        "Your boss is impressed with your spam detector and assigns you a new task. As part of improving internal tools, the company wants a search engine that can search through SMS messages and rank them by relevance. Implement the PageRank algorithm from scratch to score each SMS message based on its importance in the document graph.\n",
        "\n",
        "*   Compute TF-IDF vectors for all SMS messages (you can leverage previous implementation)\n",
        "*   Construct a document graph, where each node represents an SMS message and edges are the links between nodes.\n",
        "*  Implement the PageRank algorithm from scratch to assign an importance score to each SMS message based on its position in the document graph.\n",
        "\n",
        "#### Hint : You can use the previous dataset or any dataset from your choice.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "G_2IblEnUeju",
      "metadata": {
        "id": "G_2IblEnUeju"
      },
      "source": [
        "## You might need the follwoing formulas for your implementation\n",
        "\n",
        "---\n",
        "\n",
        "### 1) Cosine Similarity Between Two Document Vectors\n",
        "\n",
        "Cosine similarity measures how similar two vectors are based on the angle between them:\n",
        "\n",
        "$$\n",
        "\\text{cosine\\_sim}(A, B) = \\frac{A \\cdot B}{\\|A\\| \\cdot \\|B\\|}\n",
        "$$\n",
        "\n",
        "- \\( A \\cdot B \\): Dot product of vectors \\( A \\) and \\( B \\)  \n",
        "- \\( \\|A\\| \\): Euclidean norm (magnitude) of vector \\( A \\)  \n",
        "- \\( \\|B\\| \\): Euclidean norm of vector \\( B \\)\n",
        "\n",
        "**Use case**: Comparing TF-IDF vectors to measure similarity between two messages.\n",
        "\n",
        "---\n",
        "\n",
        "### 2) PageRank of a Node \\( i \\)\n",
        "\n",
        "PageRank estimates the importance of a document based on its connections in a graph:\n",
        "\n",
        "$$\n",
        "PR(i) = \\frac{1 - d}{N} + d \\sum_{j \\in M(i)} \\frac{PR(j)}{L(j)}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- \\( PR(i) \\): PageRank score of node \\( i \\)  \n",
        "- \\( d \\): Damping factor (typically 0.85)  \n",
        "- \\( N \\): Total number of nodes (documents) in the graph  \n",
        "- \\( M(i) \\): Set of nodes that link to node \\( i \\)  \n",
        "- \\( L(j) \\): Number of outbound links from node \\( j \\)  \n",
        "\n",
        "**Interpretation**:  \n",
        "- A document is important if **important documents link to it**.  \n",
        "- The score is split among a nodeâ€™s outbound links.  \n",
        "- The **teleportation term** $\\text(\\frac{1 - d}{N})$ accounts for random jumps, ensuring stability and fairness.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "808f70bd",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to C:\\Users\\FATIMA\n",
            "[nltk_data]     BAIG\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 10 Important Messages:\n",
            "\n",
            "      pagerank                          message\n",
            "4856  0.001483                     Same to u...\n",
            "1553  0.001483                         U too...\n",
            "5173  0.001207                             U 2.\n",
            "1394  0.001150                          Oh ok..\n",
            "1834  0.001132         When should I come over?\n",
            "502   0.001132             When can Ã¼ come out?\n",
            "2621  0.001132                        How come?\n",
            "1504  0.001130  Ill be there on  &lt;#&gt;  ok.\n",
            "2537  0.001069         You do what all you like\n",
            "1273  0.001056                            Ok...\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from math import log\n",
        "from collections import defaultdict\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "# Load and preprocess data\n",
        "def preprocess(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^a-z0-9\\s]\", \"\", text)\n",
        "    tokens = [word for word in text.split() if word not in stop_words]\n",
        "    return tokens\n",
        "\n",
        "def compute_tf_idf(docs):\n",
        "    tf_idf_vectors = []\n",
        "    N = len(docs)\n",
        "    df = defaultdict(int)\n",
        "\n",
        "    for doc in docs:\n",
        "        unique_terms = set(doc)\n",
        "        for term in unique_terms:\n",
        "            df[term] += 1\n",
        "\n",
        "    for doc in docs:\n",
        "        tf_idf = {}\n",
        "        term_counts = defaultdict(int)\n",
        "        for term in doc:\n",
        "            term_counts[term] += 1\n",
        "        doc_len = len(doc)\n",
        "        for term, count in term_counts.items():\n",
        "            tf = count / doc_len\n",
        "            idf = log(N / (1 + df[term]))\n",
        "            tf_idf[term] = tf * idf\n",
        "        tf_idf_vectors.append(tf_idf)\n",
        "\n",
        "    return tf_idf_vectors\n",
        "\n",
        "def cosine_similarity(vec1, vec2):\n",
        "    common = set(vec1.keys()) & set(vec2.keys())\n",
        "    dot_product = sum(vec1[x] * vec2[x] for x in common)\n",
        "    norm1 = np.sqrt(sum(v**2 for v in vec1.values()))\n",
        "    norm2 = np.sqrt(sum(v**2 for v in vec2.values()))\n",
        "    if norm1 == 0 or norm2 == 0:\n",
        "        return 0.0\n",
        "    return dot_product / (norm1 * norm2)\n",
        "\n",
        "def build_graph(tf_idf_vectors, threshold=0.2):\n",
        "    N = len(tf_idf_vectors)\n",
        "    graph = [[] for _ in range(N)]\n",
        "    for i in range(N):\n",
        "        for j in range(N):\n",
        "            if i != j:\n",
        "                sim = cosine_similarity(tf_idf_vectors[i], tf_idf_vectors[j])\n",
        "                if sim > threshold:\n",
        "                    graph[i].append(j)\n",
        "    return graph\n",
        "\n",
        "def page_rank(graph, d=0.85, max_iter=100, tol=1e-6):\n",
        "    N = len(graph)\n",
        "    pr = [1/N] * N\n",
        "\n",
        "    for _ in range(max_iter):\n",
        "        new_pr = [ (1 - d) / N ] * N\n",
        "        for i in range(N):\n",
        "            if graph[i]:\n",
        "                for j in graph[i]:\n",
        "                    new_pr[j] += d * (pr[i] / len(graph[i]))\n",
        "        if max(abs(new_pr[i] - pr[i]) for i in range(N)) < tol:\n",
        "            break\n",
        "        pr = new_pr\n",
        "    return pr\n",
        "\n",
        "def main():\n",
        "    df = pd.read_csv(r\"sms_spam_collection\\SMSSpamCollection.csv\", sep='\\t', header=None, names=['label', 'message'])\n",
        "    df['tokens'] = df['message'].apply(preprocess)\n",
        "    tf_idf_vectors = compute_tf_idf(df['tokens'])\n",
        "    graph = build_graph(tf_idf_vectors)\n",
        "    scores = page_rank(graph)\n",
        "    df['pagerank'] = scores\n",
        "    top = df.sort_values('pagerank', ascending=False).head(10)\n",
        "    print(\"Top 10 Important Messages:\\n\")\n",
        "    print(top[['pagerank', 'message']])\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import nltk\n",
        "    nltk.download('stopwords')\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55cbc82a",
      "metadata": {
        "id": "55cbc82a"
      },
      "source": [
        "### Additional Experiments *(5 additional points - <span style=\"color: red;\">Optional</span>)*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b5820d4",
      "metadata": {
        "id": "9b5820d4"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
